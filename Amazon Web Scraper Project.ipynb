{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9984dd73-086d-4bdc-a539-9fb6f13e8b8a",
   "metadata": {},
   "source": [
    "# Amazon Web Scraper Project\n",
    "\n",
    "## Objective:\n",
    "   ### This project demonstrates how to scrape product data from Amazon using Python and the BeautifulSoup library. It retrieves product details such as        titles and prices from an Amazon search results page.\n",
    "\n",
    "## Disclaimer: \n",
    "   ### This project is for educational purposes only. Scraping Amazon violates their Terms of Service, and should not be used in real-world applications  without explicit permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a759bcbf-72b7-4dea-aad7-38683350e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup  # BeautifulSoup helps in parsing HTML and extracting data\n",
    "import requests  # Requests allow sending HTTP requests to fetch web pages\n",
    "import time  # Time module for adding delays if needed\n",
    "import datetime  # Datetime module for handling date and time operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14564d6f-97c9-4684-ab48-27bd23965151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seek Buy Love Relax Spreadsheet Funny T-Shirt, Accountant Financial Analyst Tee, Casual Office Humor Graphic Shirt, Unisex Cotton Top\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "# Connect to Website\n",
    "\n",
    "url = 'https://www.amazon.in/Seek-Buy-Love-Spreadsheet-Accountant/dp/B0D29Q3DRD/ref=sr_1_11?sr=8-11&psc=1'  # URL of the product page\n",
    "\n",
    "# Define request headers to mimic a browser request and avoid bot detection\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Send an HTTP request to fetch the webpage\n",
    "page = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the page content using BeautifulSoup\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")  # Initial parsing of HTML content\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")  # Clean up formatting\n",
    "\n",
    "# Extract product title and remove any leading/trailing whitespace\n",
    "title = soup2.find(id='productTitle').get_text().strip()\n",
    "\n",
    "# Extract product price from the specific class\n",
    "price = soup2.find(class_='a-price-whole').get_text().strip()\n",
    "\n",
    "# Print the extracted information\n",
    "print(title)\n",
    "print(price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa05822-4c49-4d47-89a2-ac8869207339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-26\n"
     ]
    }
   ],
   "source": [
    "import datetime  # Import the datetime module\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(today)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19cf40f2-a4a2-4cef-a256-0040979fe715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # Import the CSV module for handling CSV file operations\n",
    "\n",
    "# Define the header row for the CSV file\n",
    "header = ['Title', 'Price', 'Date']\n",
    "\n",
    "# Store extracted data (title, price, and today's date) in a list\n",
    "data = [title, price, today]\n",
    "\n",
    "# Open a new CSV file in write mode with UTF-8 encoding to avoid special character issues\n",
    "with open('AmazonWebScraperProject.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)  # Create a CSV writer object\n",
    "    writer.writerow(header)  # Write the header row\n",
    "    writer.writerow(data)  # Write the product data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773f2c2-891f-409f-b7ad-2b8f2958765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas for handling data\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(r'C:\\Users\\asmza\\AmazonWebScraperProject.csv')\n",
    "\n",
    "# Print the contents of the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4b4e483-b5fd-4a7e-b934-764be0f762b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are appending the data to the CSV file\n",
    "\n",
    "with open('AmazonWebScraperProject.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)  # Create a CSV writer object\n",
    "    writer.writerow(data)  # Append the new product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbd830b-79c6-4013-b769-6ab7ce066256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_price():\n",
    "    # Define the URL of the Amazon product page\n",
    "    url = 'https://www.amazon.in/Seek-Buy-Love-Spreadsheet-Accountant/dp/B0D29Q3DRD/ref=sr_1_11?sr=8-11&psc=1'\n",
    "\n",
    "    # Set up headers to mimic a real browser request and avoid bot detection\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\"}\n",
    "\n",
    "    # Send an HTTP request to fetch the webpage\n",
    "    page = requests.get(url, headers=headers)\n",
    "\n",
    "    # Parse the page content using BeautifulSoup\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")  # Initial parsing\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")  # Clean up HTML formatting\n",
    "\n",
    "    # Extract the product title and price\n",
    "    title = soup2.find(id='productTitle').get_text().strip()\n",
    "    price = soup2.find(class_='a-price-whole').get_text().strip()\n",
    "\n",
    "    # Get today's date\n",
    "    import datetime\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    # Prepare data for CSV storage\n",
    "    import csv\n",
    "    header = ['Title', 'Price', 'Date']\n",
    "    data = [title, price, today]\n",
    "\n",
    "    # Open CSV file in append mode to add new data\n",
    "    with open('AmazonWebScraperProject.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)  # Append new product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06fab4-de2b-4d7c-8238-3922488824d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    check_price()  # Call the function to scrape and store the price\n",
    "    time.sleep(86400)  # Pause execution for 24 hours (86400 seconds) before running again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c65d6a-a63a-4b12-92bd-8c0d3dc272aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e2169-9a53-44f0-ab35-6f76c795afee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540f321-8ee9-49bc-a85b-717ded668469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa1b89-a109-4bcf-8cd5-5cbad36c54a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
